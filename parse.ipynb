{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "5c320f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from typing import NamedTuple, List\n",
    "\n",
    "class Action(NamedTuple):\n",
    "    date: date\n",
    "    action: str\n",
    "\n",
    "class Run(NamedTuple):\n",
    "    date: date\n",
    "    run: int\n",
    "    precursor: str\n",
    "    co_reactant: str\n",
    "    co_absorbate: str | None\n",
    "    cycles: int\n",
    "    process: str\n",
    "    sequence: str\n",
    "    T: int\n",
    "    P: int\n",
    "\n",
    "ACTIONS: List[Action] = []\n",
    "RUNS: List[Run] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "640bda44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import io\n",
    "from parse import parse, Result, with_pattern\n",
    "import string\n",
    "from typing import cast\n",
    "\n",
    "def parse_experiment_contents(buffer: io.TextIOBase, global_date: date):\n",
    "    global_T: int\n",
    "    global_P: int\n",
    "\n",
    "    buffer.seek(0)\n",
    "    \n",
    "    # Register Globals\n",
    "    _result = find_read_line(buffer, \"T{:ws}={:ws}{T:g}{:ws}{T_unit:l}\", True)\n",
    "    if not \"T\" in _result:\n",
    "        raise ValueError(\"Could not find global temperature!\")\n",
    "    global_T = _result[\"T\"]\n",
    "    \n",
    "    _result = find_read_line_aliases(buffer, [\n",
    "        \"P{:ws}={:ws}{P:g}{:ws}{P_unit:l}\",\n",
    "        \"Pressure{:ws}={:ws}{P:g}{:ws}{P_unit:l}\"\n",
    "    ], True)\n",
    "    if not \"P\" in _result:\n",
    "        raise ValueError(\"Could not find global pressure!\")\n",
    "    global_P = _result[\"P\"]\n",
    "\n",
    "    # Actions\n",
    "    while True:\n",
    "        _checkpoint_pre_action = buffer.tell()\n",
    "        if not find_read_separator(buffer, \"_\", 28):\n",
    "            raise ValueError(\"Could not find action section\")\n",
    "\n",
    "        if occurs_before_separator(buffer, \"loaded\", \"_\", 28) != -1:\n",
    "            action_label = str(read_nonws_line(buffer))  # Cannot be None / EOF if condition is true\n",
    "            if \"loaded\" in action_label:\n",
    "                _label_segments = action_label.partition(\"loaded\")\n",
    "\n",
    "                action_label = _label_segments[0]\n",
    "                _action_date_str = _label_segments[2].strip()\n",
    "                \n",
    "                _result = parse_aliases_to_dict([\n",
    "                    \"{month:d}/{day:d}/{year:d}\",\n",
    "                    \"{month:d}/{day:d}\",\n",
    "                    \"{year:4d}{month:2d}{day:2d}\",\n",
    "                ], _action_date_str, True)\n",
    "            else: \n",
    "                _result = find_read_line_aliases(buffer, [\n",
    "                    \"loaded{:ws}{month:d}/{day:d}/{year:d}\",\n",
    "                    \"loaded{:ws}{month:d}/{day:d}\",\n",
    "                    \"loaded{:ws}{year:4d}{month:2d}{day:2d}\",\n",
    "                ], True)\n",
    "            \n",
    "            if len(_result) == 0:\n",
    "                raise ValueError(\"Failed to parse load date!\")\n",
    "            \n",
    "            if \"year\" not in _result:\n",
    "                _result[\"year\"] = global_date.year\n",
    "\n",
    "            action_date: date = datetime(\n",
    "                year=_result[\"year\"],\n",
    "                month=_result[\"month\"],\n",
    "                day=_result[\"day\"],\n",
    "            )\n",
    "            # Register Action\n",
    "            this_action = Action(\n",
    "                date=action_date,\n",
    "                action=f\"{action_label} loaded\"\n",
    "            )\n",
    "            ACTIONS.append(this_action)\n",
    "        else:\n",
    "            # Not an action, likely a run instead\n",
    "            print(f\"{buffer.tell()}: not an action, likely a run instead\")\n",
    "            buffer.seek(_checkpoint_pre_action)\n",
    "            break\n",
    "\n",
    "    # Runs\n",
    "    while True:\n",
    "        if find_read_separator(buffer, \"_\", 28) == -1:\n",
    "            break \n",
    "\n",
    "        _result = find_read_line(buffer, \"Run{:ws}{number:d}\", True)\n",
    "        if not \"number\" in _result:\n",
    "            break\n",
    "        \n",
    "        run_number = _result[\"number\"]\n",
    "\n",
    "        _result = find_read_line_aliases(buffer, [\n",
    "            \"{cycles:d}{:ws}cycles{:ws}{precursor}|{co_reactant}\",\n",
    "            \"{cycles:d}{:ws}cycles{:ws}{precursor}{:ws}and{:ws}{co_reactant}{:s}{co_absorbate}\",\n",
    "        ], True)\n",
    "        run_cycles = _result[\"cycles\"]\n",
    "        run_precursor = _result[\"precursor\"]\n",
    "        run_co_reactant = _result[\"co_reactant\"]\n",
    "        run_co_absorbate = None\n",
    "        \n",
    "        if \"co_absorbate\" in _result:\n",
    "            run_co_absorbate = _result[\"co_absorbate\"]\n",
    "\n",
    "        run_process = read_nonws_line(buffer)\n",
    "        if not run_process:\n",
    "            raise ValueError(f\"Missing run process\")\n",
    "        _run_proc_partitions = run_process.count(\"|\") \n",
    "\n",
    "        run_sequence = read_nonws_line(buffer)\n",
    "        if not run_sequence:\n",
    "            raise ValueError(f\"Missing run sequence\")\n",
    "        _run_seq_partitions = run_sequence.count(\"|\")\n",
    "        \n",
    "        if (_run_proc_partitions != _run_seq_partitions):\n",
    "            raise ValueError(f\"Mismatch in partitions between process and run sequence in run #{run_number}\")\n",
    "        \n",
    "        # Register Run\n",
    "        this_run = Run(\n",
    "            date=global_date,\n",
    "            run=run_number,\n",
    "            precursor=run_precursor,\n",
    "            co_reactant=run_co_reactant,\n",
    "            co_absorbate=run_co_absorbate,\n",
    "            cycles=run_cycles,\n",
    "            process=run_process,\n",
    "            sequence=run_sequence,\n",
    "            T=global_T,\n",
    "            P=global_P\n",
    "        )\n",
    "        RUNS.append(this_run)\n",
    "       \n",
    "def occurs_before_separator(buffer: io.TextIOBase, key: str, tile: str, threshold: int) -> int:\n",
    "    checkpoint = buffer.tell()\n",
    "\n",
    "    while True:\n",
    "        line = read_nonws_line(buffer)\n",
    "        if not line:\n",
    "            return -1\n",
    "        \n",
    "        if not key in line:\n",
    "            continue\n",
    "            \n",
    "        result = buffer.tell()\n",
    "        buffer.seek(checkpoint)\n",
    "        return result\n",
    "\n",
    "def find_read_separator(buffer: io.TextIOBase, tile: str, threshold: int) -> int:\n",
    "    threshold_str = tile * (threshold-1)\n",
    "\n",
    "    result = find_read_line(buffer, threshold_str + \"{remainder:w}\", True)\n",
    "    if \"__file_position__\" not in result:\n",
    "        return -1\n",
    "    \n",
    "    return result[\"__file_position__\"]\n",
    "\n",
    "def find_read_line(buffer: io.TextIOBase, pattern: str, accept_ws_pad: bool) -> dict:\n",
    "    return find_read_line_aliases(buffer, [pattern], accept_ws_pad)\n",
    "\n",
    "def find_read_line_aliases(buffer: io.TextIOBase, patterns: List[str], accept_ws_pad: bool) -> dict:\n",
    "    checkpoint = buffer.tell()\n",
    "\n",
    "    while True:\n",
    "        line = read_nonws_line(buffer)\n",
    "        if not line:\n",
    "            print(f\"Could not find line: {patterns}\")\n",
    "            buffer.seek(checkpoint)\n",
    "            return {}\n",
    "\n",
    "        result_dict = parse_aliases_to_dict(patterns, line.strip(), accept_ws_pad)\n",
    "        if len(result_dict) == 0:\n",
    "            continue\n",
    "        \n",
    "        result_dict[\"__file_position__\"] = buffer.tell()\n",
    "        return result_dict\n",
    "\n",
    "def parse_aliases_to_dict(patterns: List[str], line: str, accept_ws_pad: bool) -> dict:\n",
    "    for pattern in patterns:\n",
    "        result = parse_to_dict(pattern, line, accept_ws_pad)\n",
    "        if len(result) != 0:\n",
    "            return result\n",
    "    return {}\n",
    "\n",
    "def parse_to_dict(pattern: str, line: str, accept_ws_pad: bool) -> dict:\n",
    "    @with_pattern(r\"\\s*\")\n",
    "    def whitespace(text):\n",
    "        return text\n",
    "\n",
    "\n",
    "    if (accept_ws_pad):\n",
    "        pattern = \"{:ws}\" + pattern\n",
    "\n",
    "    result = parse(pattern, line, extra_types={\"ws\": whitespace})\n",
    "    if not result:\n",
    "        return {}\n",
    "    \n",
    "    result = cast(Result, result) \n",
    "\n",
    "    formatter = string.Formatter()\n",
    "    field_names = [fname for _, fname, _, _ in formatter.parse(pattern) if fname]\n",
    "\n",
    "    # Check that all named fields are non-empty\n",
    "    if not all(result.named.get(name) not in [None, \"\"] for name in field_names):\n",
    "        print(f\"A parameter was missing that fit {pattern}\")\n",
    "        return {}\n",
    "\n",
    "    result_dict = result.named\n",
    "    result_dict[\"__match_pattern__\"] = pattern\n",
    "    return result_dict\n",
    "\n",
    "\n",
    "def read_nonws_line(buffer: io.TextIOBase) -> str | None: \n",
    "    checkpoint = buffer.tell()\n",
    "    \n",
    "    while True:\n",
    "        line = buffer.readline()\n",
    "        if not line:\n",
    "            buffer.seek(checkpoint)\n",
    "            return None\n",
    "\n",
    "        if not line.strip():\n",
    "            continue\n",
    "\n",
    "        return line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "1ae6b092",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "def parse_experiment_file(path):\n",
    "    f = open(path, 'r')\n",
    "    fn = os.path.basename(path)\n",
    "    date_str = fn[0:8]\n",
    "    \n",
    "    experiment_date: date = datetime.strptime(date_str, \"%Y%m%d\")\n",
    "    parse_experiment_contents(f, experiment_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "ea21803a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def update_experiment_df(df: pd.DataFrame):\n",
    "    for action in ACTIONS:\n",
    "        action_row = pd.DataFrame([{\n",
    "            'Date': int(action.date.strftime(\"%Y%m%d\")),\n",
    "            'Run/Action': action.action,\n",
    "            'Precursor': None,\n",
    "            'Co-reactant': None,\n",
    "            'Co-absorbate': None,\n",
    "            '# Cycles': None,\n",
    "            'Process': None,\n",
    "            'Sequence': None,\n",
    "            'Furnace T (°C)': None,\n",
    "            'P (Torr)': None,\n",
    "            'Crystal substrate': None,\n",
    "        }])\n",
    "\n",
    "        df = pd.concat([df, action_row], ignore_index=True)\n",
    "\n",
    "    for run in RUNS:\n",
    "        _run_co_absorbate = run.co_absorbate\n",
    "        if not _run_co_absorbate:\n",
    "            _run_co_absorbate = \"\"\n",
    "\n",
    "        run_row = pd.DataFrame([{\n",
    "            'Date': int(run.date.strftime(\"%Y%m%d\")),\n",
    "            'Run/Action': run.run,\n",
    "            'Precursor': run.precursor,\n",
    "            'Co-reactant': run.co_reactant,\n",
    "            'Co-absorbate': None,\n",
    "            '# Cycles': None,\n",
    "            'Process': f\"{run.cycles} cycles {run.process} {_run_co_absorbate}\",\n",
    "            'Sequence': run.sequence,\n",
    "            'Furnace T (°C)': run.T,\n",
    "            'P (Torr)': run.P,\n",
    "            'Crystal substrate': None,\n",
    "        }])\n",
    "\n",
    "        df = pd.concat([df, run_row], ignore_index=True)\n",
    "\n",
    "    df = df.sort_values(by=['Date', 'Run/Action'], ascending=[True, True])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "f9123d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\20220926_notes.txt\n",
      "1513: not an action, likely a run instead\n",
      "Could not find line: ['Run{:ws}{number:d}']\n",
      "data\\20220927_notes.txt\n",
      "1169: not an action, likely a run instead\n",
      "Could not find line: ['Run{:ws}{number:d}']\n",
      "data\\20220928_notes.txt\n",
      "1167: not an action, likely a run instead\n",
      "Could not find line: ['Run{:ws}{number:d}']\n",
      "data\\20220929_notes.txt\n",
      "1638: not an action, likely a run instead\n",
      "Could not find line: ['Run{:ws}{number:d}']\n",
      "data\\20220930_notes.txt\n",
      "1176: not an action, likely a run instead\n",
      "Could not find line: ['Run{:ws}{number:d}']\n",
      "data\\20240123.txt\n",
      "947: not an action, likely a run instead\n",
      "Could not find line: ['___________________________{remainder:w}']\n",
      "data\\20240124.txt\n",
      "643: not an action, likely a run instead\n",
      "Could not find line: ['Run{:ws}{number:d}']\n",
      "data\\20240125.txt\n",
      "965: not an action, likely a run instead\n",
      "Could not find line: ['___________________________{remainder:w}']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fiery\\AppData\\Local\\Temp\\ipykernel_22948\\3106128970.py:40: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, run_row], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# parse_experiment_file('data/20240123.txt')\n",
    "# parse_experiment_file('data/20240124.txt')\n",
    "# parse_experiment_file('data/20240125.txt')\n",
    "# parse_experiment_file('data/20220928_notes.txt')\n",
    "\n",
    "import glob\n",
    "\n",
    "for filename in glob.glob(os.path.join('data', '*.txt')):\n",
    "   print(filename)\n",
    "   parse_experiment_file(filename)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df = update_experiment_df(df)\n",
    "df.to_excel('output.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lynden",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
