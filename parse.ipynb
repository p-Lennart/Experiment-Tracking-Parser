{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c320f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from typing import NamedTuple, List\n",
    "\n",
    "class Action(NamedTuple):\n",
    "    date: date\n",
    "    action: str\n",
    "\n",
    "class Run(NamedTuple):\n",
    "    date: date\n",
    "    run: int\n",
    "    precursor: str\n",
    "    co_reactant: str\n",
    "    cycles: int\n",
    "    process: str\n",
    "    sequence: str\n",
    "    T: int\n",
    "    P: int\n",
    "\n",
    "ACTIONS: List[Action] = []\n",
    "RUNS: List[Run] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640bda44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import io\n",
    "from parse import parse, Result\n",
    "import string\n",
    "from typing import cast\n",
    "\n",
    "def parse_experiment_contents(buffer: io.TextIOBase, global_date: date):\n",
    "    global_T: int\n",
    "    global_P: int\n",
    "\n",
    "    buffer.seek(0)\n",
    "    \n",
    "    # Register Globals\n",
    "    _result = find_read_line(buffer, \"T{:s}={:s}{T:g}{T_unit:l}\")\n",
    "    if not \"T\" in _result:\n",
    "        raise ValueError(\"Could not find global temperature!\")\n",
    "    global_T = _result[\"T\"]\n",
    "    \n",
    "    _result = find_read_line_aliases(buffer, [\n",
    "        \"P{:s}={:s}{P:g}{:s}{P_unit:l}\",\n",
    "        \"Pressure{:s}={:s}{P:g}{:s}{P_unit:l}\"\n",
    "    ])\n",
    "    if not \"P\" in _result:\n",
    "        raise ValueError(\"Could not find global pressure!\")\n",
    "    global_P = _result[\"P\"]\n",
    "\n",
    "    # Action\n",
    "    _checkpoint_pre_action = buffer.tell()\n",
    "    if not find_read_separator(buffer, \"_\", 28):\n",
    "        raise ValueError(\"Could not find action section\")\n",
    "\n",
    "    if occurs_before_separator(buffer, \"loaded\", \"_\", 28):\n",
    "        action_label = read_nonws_line(buffer)\n",
    "        \n",
    "        _result = find_read_line(buffer, \"{action_type} {month:d}/{day:d}/{year:d}\")\n",
    "        action_type = _result[\"action_type\"]\n",
    "        action_date: date = datetime(\n",
    "            year=_result[\"year\"],\n",
    "            month=_result[\"month\"],\n",
    "            day=_result[\"day\"]\n",
    "        )\n",
    "\n",
    "        # Action: Wafer\n",
    "        if find_read_separator(buffer, \"_\", 28) == -1:\n",
    "            raise ValueError(\"Could not find action->wafer section\")\n",
    "        \n",
    "        wafer_label = read_nonws_line(buffer)\n",
    "        # discard wafer label for now\n",
    "        \n",
    "        # Register Action\n",
    "        this_action = Action(\n",
    "            date=action_date,\n",
    "            action=f\"{action_label} {action_type}\"\n",
    "        )\n",
    "        ACTIONS.append(this_action)\n",
    "    else:\n",
    "        # Not an action, likely a run instead\n",
    "        buffer.seek(_checkpoint_pre_action)\n",
    "\n",
    "    # Runs\n",
    "    while True:\n",
    "        if find_read_separator(buffer, \"_\", 28) == -1:\n",
    "            break \n",
    "\n",
    "        _result = find_read_line(buffer, \"Run{:s}{number:d}\")\n",
    "        if not \"number\" in _result:\n",
    "            break\n",
    "        \n",
    "        run_number = _result[\"number\"]\n",
    "\n",
    "        _result = find_read_line(buffer, \"{cycles:d}{:s}cycles{:s}{precursor}|{co_reactant}\")\n",
    "        run_cycles = _result[\"cycles\"]\n",
    "        run_precursor = _result[\"precursor\"]\n",
    "        run_co_reactant = _result[\"co_reactant\"]\n",
    "\n",
    "        run_process = read_nonws_line(buffer)\n",
    "        if not run_process:\n",
    "            raise ValueError(f\"Missing run process\")\n",
    "        _run_proc_partitions = run_process.count(\"|\") \n",
    "\n",
    "        run_sequence = read_nonws_line(buffer)\n",
    "        if not run_sequence:\n",
    "            raise ValueError(f\"Missing run sequence\")\n",
    "        _run_seq_partitions = run_sequence.count(\"|\")\n",
    "        \n",
    "        if (_run_proc_partitions != _run_seq_partitions):\n",
    "            raise ValueError(f\"Mismatch in partitions between process and run sequence in run #{run_number}\")\n",
    "        \n",
    "        # Register Run\n",
    "        this_run = Run(\n",
    "            date=global_date,\n",
    "            run=run_number,\n",
    "            precursor=run_precursor,\n",
    "            co_reactant=run_co_reactant,\n",
    "            cycles=run_cycles,\n",
    "            process=run_process,\n",
    "            sequence=run_sequence,\n",
    "            T=global_T,\n",
    "            P=global_P\n",
    "        )\n",
    "        RUNS.append(this_run)\n",
    "       \n",
    "def occurs_before_separator(buffer: io.TextIOBase, key: str, tile: str, threshold: int) -> int:\n",
    "    checkpoint = buffer.tell()\n",
    "\n",
    "    while True:\n",
    "        line = read_nonws_line(buffer)\n",
    "        if not line:\n",
    "            return -1\n",
    "        \n",
    "        if not key in line:\n",
    "            continue\n",
    "            \n",
    "        result = buffer.tell()\n",
    "        buffer.seek(checkpoint)\n",
    "        return result\n",
    "\n",
    "def find_read_separator(buffer: io.TextIOBase, tile: str, threshold: int) -> int:\n",
    "    threshold_str = tile * (threshold-1)\n",
    "\n",
    "    result = find_read_line(buffer, threshold_str + \"{remainder:w}\")\n",
    "    if \"__file_position__\" not in result:\n",
    "        return -1\n",
    "    \n",
    "    return result[\"__file_position__\"]\n",
    "\n",
    "def find_read_line(buffer: io.TextIOBase, pattern: str) -> dict:\n",
    "    return find_read_line_aliases(buffer, [pattern])\n",
    "\n",
    "def find_read_line_aliases(buffer: io.TextIOBase, patterns: List[str]) -> dict:\n",
    "    checkpoint = buffer.tell()\n",
    "\n",
    "    while True:\n",
    "        line = read_nonws_line(buffer)\n",
    "        if not line:\n",
    "            print(f\"Could not find line: {patterns}\")\n",
    "            buffer.seek(checkpoint)\n",
    "            return {}\n",
    "\n",
    "        result_dict = parse_aliases_to_dict(patterns, line.strip())\n",
    "        if len(result_dict) == 0:\n",
    "            continue\n",
    "        \n",
    "        result_dict[\"__file_position__\"] = buffer.tell()\n",
    "        return result_dict\n",
    "\n",
    "def parse_aliases_to_dict(patterns, line) -> dict:\n",
    "    for pattern in patterns:\n",
    "        result = parse_to_dict(pattern, line)\n",
    "        if len(result) != 0:\n",
    "            return result\n",
    "    return {}\n",
    "\n",
    "\n",
    "def parse_to_dict(pattern, line) -> dict:\n",
    "    result = parse(pattern, line)\n",
    "    if not result:\n",
    "        return {}\n",
    "    \n",
    "    result = cast(Result, result) \n",
    "\n",
    "    formatter = string.Formatter()\n",
    "    field_names = [fname for _, fname, _, _ in formatter.parse(pattern) if fname]\n",
    "\n",
    "    # Check that all named fields are non-empty\n",
    "    if not all(result.named.get(name) not in [None, \"\"] for name in field_names):\n",
    "        print(f\"A parameter was missing that fit {pattern}\")\n",
    "        return {}\n",
    "\n",
    "    result_dict = result.named\n",
    "    result_dict[\"__match_pattern__\"] = pattern\n",
    "    return result_dict\n",
    "\n",
    "\n",
    "def read_nonws_line(buffer: io.TextIOBase) -> str | None: \n",
    "    checkpoint = buffer.tell()\n",
    "    \n",
    "    while True:\n",
    "        line = buffer.readline()\n",
    "        if not line:\n",
    "            buffer.seek(checkpoint)\n",
    "            return None\n",
    "\n",
    "        if not line.strip():\n",
    "            continue\n",
    "\n",
    "        return line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ae6b092",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "def parse_experiment_file(path):\n",
    "    f = open(path, 'r')\n",
    "    exp_date: date = datetime.strptime(os.path.basename(path), \"%Y%m%d.txt\")\n",
    "    parse_experiment_contents(f, exp_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea21803a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bisect\n",
    "import pandas as pd\n",
    "\n",
    "def update_experiment_df(df: pd.DataFrame):\n",
    "    for action in ACTIONS:\n",
    "        action_row = pd.DataFrame([{\n",
    "            'Date': int(action.date.strftime(\"%Y%m%d\")),\n",
    "            'Run/Action': action.action,\n",
    "            'Precursor': None,\n",
    "            'Co-reactant': None,\n",
    "            'Co-absorbate': None,\n",
    "            '# Cycles': None,\n",
    "            'Process': None,\n",
    "            'Sequence': None,\n",
    "            'Furnace T (°C)': None,\n",
    "            'P (Torr)': None,\n",
    "        }])\n",
    "\n",
    "        df = pd.concat([df, action_row], ignore_index=True)\n",
    "\n",
    "    for run in RUNS:\n",
    "        run_row = pd.DataFrame([{\n",
    "            'Date': int(run.date.strftime(\"%Y%m%d\")),\n",
    "            'Run/Action': run.run,\n",
    "            'Precursor': run.precursor,\n",
    "            'Co-reactant': run.co_reactant,\n",
    "            'Co-absorbate': None,\n",
    "            '# Cycles': None,\n",
    "            'Process': f\"{run.cycles} cycles {run.process}\",\n",
    "            'Sequence': run.sequence,\n",
    "            'Furnace T (°C)': run.T,\n",
    "            'P (Torr)': run.P,\n",
    "        }])\n",
    "\n",
    "        df = pd.concat([df, run_row], ignore_index=True)\n",
    "\n",
    "    df = df.sort_values(by=['Date', 'Run/Action'], ascending=[True, True])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9123d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find line: ['___________________________{remainder:w}']\n",
      "Could not find line: ['Run{:s}{number:d}']\n",
      "Could not find line: ['___________________________{remainder:w}']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fiery\\AppData\\Local\\Temp\\ipykernel_9900\\1201117729.py:35: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, run_row], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "parse_experiment_file('data/20240123.txt')\n",
    "parse_experiment_file('data/20240124.txt')\n",
    "parse_experiment_file('data/20240125.txt')\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df = update_experiment_df(df)\n",
    "df.to_excel('output.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lynden",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
